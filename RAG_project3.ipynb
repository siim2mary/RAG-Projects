{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91ac7d3324ac499db069fae043337a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc40333be5eb4bfbbf4ec7ce6ad71b85"
            ],
            "layout": "IPY_MODEL_7a0a33032432416f8b2bf20ce9cab968"
          }
        },
        "f3a3aaca79fc406e88cc9da643e90a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42111d5a9ad645cbb980a0f369ac8325",
            "placeholder": "​",
            "style": "IPY_MODEL_d04faf58e9be43918a3af0542d491ae2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "718861943f8b47639eee22fadca8f9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dd61164e648b44a589c2730065c8a281",
            "placeholder": "​",
            "style": "IPY_MODEL_c265f762c9214dba8673f54e466b6453",
            "value": ""
          }
        },
        "d959fa8ba58b4dc1bfe2165f1901bc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4a4c4202d8ac44398b6f63f10b7de47a",
            "style": "IPY_MODEL_ce3e37f3128a4ecdb8cf540b8fe8b39e",
            "value": true
          }
        },
        "98da00c390974ba4979c475cd9be0698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1d86a2d96c6b4c18be818e0bd9a7d3c2",
            "style": "IPY_MODEL_321be4c5a5fc40ffa9d5f1fb8c883dc6",
            "tooltip": ""
          }
        },
        "b5f4f39ffc364340a63d0ca80f7472c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d4283bd8974fd2882ddf8e94c9bb6a",
            "placeholder": "​",
            "style": "IPY_MODEL_f215edb2000348ad807a14c653dc5295",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7a0a33032432416f8b2bf20ce9cab968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "42111d5a9ad645cbb980a0f369ac8325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04faf58e9be43918a3af0542d491ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd61164e648b44a589c2730065c8a281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c265f762c9214dba8673f54e466b6453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a4c4202d8ac44398b6f63f10b7de47a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3e37f3128a4ecdb8cf540b8fe8b39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d86a2d96c6b4c18be818e0bd9a7d3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321be4c5a5fc40ffa9d5f1fb8c883dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "76d4283bd8974fd2882ddf8e94c9bb6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f215edb2000348ad807a14c653dc5295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16a0c5b7ec9c4fa0a665c8d42aaf2dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b35552ee389542e6baecfdec6eda2e61",
            "placeholder": "​",
            "style": "IPY_MODEL_b2cacd2926c745b29cc1d623e8adee90",
            "value": "Connecting..."
          }
        },
        "b35552ee389542e6baecfdec6eda2e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2cacd2926c745b29cc1d623e8adee90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4732dc32bb594da590eb6e88852aca85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd645241641346339d3da3cbf1b8fc5a",
            "placeholder": "​",
            "style": "IPY_MODEL_ccfbfb06656348f9aa0ed9efd5ad5905",
            "value": "Connecting..."
          }
        },
        "dd645241641346339d3da3cbf1b8fc5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccfbfb06656348f9aa0ed9efd5ad5905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc40333be5eb4bfbbf4ec7ce6ad71b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_265dab216650476893fe6717d4ee2b09",
            "placeholder": "​",
            "style": "IPY_MODEL_9c6537fb07704488b04717acf85b619e",
            "value": "Token GOOGLR COLAB not found in /root/.cache/huggingface/stored_tokens"
          }
        },
        "265dab216650476893fe6717d4ee2b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c6537fb07704488b04717acf85b619e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Project: RAG System for Legal Document Q&A and Clause Retrieval\n",
        "================================================================================\n",
        "# Author: simmy xavier\n",
        "# GitHub: github.com/siim2mary\n",
        "#\n",
        "#Project Overview\n",
        "#===============================================================================\n",
        "The project aims to build a Retrieval-Augmented Generation (RAG) system for answering questions about legal documents. The system uses a combination of natural language processing (NLP) and information retrieval techniques to:\n",
        "Retrieve relevant text chunks: The system searches for relevant text chunks in a dataset of contracts.\n",
        "Generate answers: The system uses a Large Language Model (LLM) to generate answers to questions based on the retrieved text chunks.\n",
        "\n",
        "#Project Components:\n",
        "#===============================================================================\n",
        "The project covers the following components:\n",
        "\n",
        "1. Loading a specialized dataset: The project uses a dataset of contracts from Hugging Face.\n",
        "2. Creating text chunks: The project creates text chunks from the contracts dataset for retrieval.\n",
        "3. Generating vector embeddings: The project generates vector embeddings for the text chunks using a sentence transformer model.\n",
        "4. Building a vector index: The project builds a vector index using FAISS (Facebook AI Similarity Search) for efficient similarity search.\n",
        "5. Implementing a RAG pipeline: The project implements a simple RAG pipeline to answer questions about legal documents.\n",
        "\n",
        "#Goals and Objectives:\n",
        "#===============================================================================\n",
        "\n",
        "The goal of the project is to create a system that can accurately answer questions about legal documents by leveraging the strengths of both information retrieval and language generation.\n",
        "\n",
        "Technical Requirements:\n",
        "==============================================================================\n",
        "The project requires the following technical components:\n",
        "1. Datasets library: To load the contracts dataset from Hugging Face.\n",
        "2. Sentence transformers library: To generate vector embeddings for the text chunks.\n",
        "3. FAISS library: To build a vector index for efficient similarity search.\n",
        "4. Langchain library: To manage the RAG pipeline.\n",
        "5. PyPDF library: To read PDF documents (optional).\n",
        "\n",
        "Overall, the project aims to demonstrate how to build a RAG system for answering questions about legal documents using open-source libraries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0jEhinSq84aF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is RAG?**\n",
        "RAG stands for Retrieval-Augmented Generation. It's a technique used in natural language processing (NLP) to generate text based on a given prompt or query. RAG combines the strengths of two approaches:\n",
        "Retrieval: This involves searching for relevant information in a large database or knowledge base.\n",
        "Generation: This involves generating text based on the retrieved information."
      ],
      "metadata": {
        "id": "BRufjDgr5XL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is RAG used?\n",
        "RAG is used to generate more accurate and informative text by leveraging the knowledge stored in a database or knowledge base. It's particularly useful for tasks like:\n",
        "\n",
        "## Question answering: RAG can retrieve relevant information and generate answers to questions.\n",
        "## Text summarization: RAG can retrieve relevant information and generate summaries of text.\n",
        "## Text generation: RAG can retrieve relevant information and generate text based on a prompt or query."
      ],
      "metadata": {
        "id": "66dnHmK55f_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What have we done so far?\n",
        "We've been working on a project that uses RAG to generate text about contracts. Specifically, we've been trying to generate responses to questions about the key elements of a contract."
      ],
      "metadata": {
        "id": "7HxCmcgn5yh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Explanation\n",
        "Let's go through the code we've written so far:"
      ],
      "metadata": {
        "id": "dUDf1uk452lH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2sT7ldK80eR",
        "outputId": "ac007009-4aca-45e5-844a-cf463cc151f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries...\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing required libraries...\")\n",
        "!pip install -q datasets sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irdMyS04rM7m",
        "outputId": "7134a173-59bd-48fa-9195-04e86b4d82cc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "T7kpyBVPrSE0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets sentence-transformers faiss-cpu langchain langchain-community pypdf accelerate bitsandbytes!pip install -q datasets sentence-transformers faiss-cpu langchain langchain-community pypdf accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "xeUm4jZurWbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e372586-e78b-47c4-a736-83f5eec90229"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'bitsandbytes!pip': Expected end or semicolon (after name and no valid version specifier)\n",
            "    bitsandbytes!pip\n",
            "                ^\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oMW8H_em_4_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries.\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SspYKFl69HZ9",
        "outputId": "14b34486-6683-4798-d1da-969641c9c7ef"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "H-tBpeO7tl89"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The RecursiveCharacterTextSplitter is used to split a text into chunks of up to 1000 characters, with an overlap of 200 characters between consecutive chunks. The separators parameter specifies that the text should be split at newline characters, spaces, or empty strings.\n",
        "Use Case\n",
        "The RecursiveCharacterTextSplitter is particularly useful in natural language processing (NLP) applications, such as:\n",
        "Text summarization: Breaking down large documents into smaller chunks for summarization.\n",
        "Question answering: Splitting text into chunks to improve the accuracy of question answering models.\n",
        "Information retrieval: Indexing and retrieving specific chunks of text from large documents.\n",
        "By using the RecursiveCharacterTextSplitter, you can efficiently process and analyze large texts, making it easier to extract insights and meaning from the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p2SfY5fWuzXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "91ac7d3324ac499db069fae043337a58",
            "f3a3aaca79fc406e88cc9da643e90a59",
            "718861943f8b47639eee22fadca8f9ce",
            "d959fa8ba58b4dc1bfe2165f1901bc57",
            "98da00c390974ba4979c475cd9be0698",
            "b5f4f39ffc364340a63d0ca80f7472c1",
            "7a0a33032432416f8b2bf20ce9cab968",
            "42111d5a9ad645cbb980a0f369ac8325",
            "d04faf58e9be43918a3af0542d491ae2",
            "dd61164e648b44a589c2730065c8a281",
            "c265f762c9214dba8673f54e466b6453",
            "4a4c4202d8ac44398b6f63f10b7de47a",
            "ce3e37f3128a4ecdb8cf540b8fe8b39e",
            "1d86a2d96c6b4c18be818e0bd9a7d3c2",
            "321be4c5a5fc40ffa9d5f1fb8c883dc6",
            "76d4283bd8974fd2882ddf8e94c9bb6a",
            "f215edb2000348ad807a14c653dc5295",
            "16a0c5b7ec9c4fa0a665c8d42aaf2dda",
            "b35552ee389542e6baecfdec6eda2e61",
            "b2cacd2926c745b29cc1d623e8adee90",
            "4732dc32bb594da590eb6e88852aca85",
            "dd645241641346339d3da3cbf1b8fc5a",
            "ccfbfb06656348f9aa0ed9efd5ad5905",
            "fc40333be5eb4bfbbf4ec7ce6ad71b85",
            "265dab216650476893fe6717d4ee2b09",
            "9c6537fb07704488b04717acf85b619e"
          ]
        },
        "id": "yv7MOdAcr0vs",
        "outputId": "50b140a4-e828-486f-b829-197ca895fd05"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91ac7d3324ac499db069fae043337a58"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlpkeAyCuJDt",
        "outputId": "3e4234a2-48bc-43e8-bf5f-01008567e377"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what we have done so far?\n",
        "Installs the necessary libraries, imports them, and loads a dataset.\n",
        "\n",
        "Library Installation\n",
        "=============================\n",
        "The code installs the following libraries:\n",
        "Datasets library: For loading datasets from Hugging Face.\n",
        "\n",
        "Sentence transformers library: For generating vector embeddings.\n",
        "\n",
        "FAISS library: For building a vector index for efficient similarity search.\n",
        "\n",
        "Langchain library: For managing the RAG pipeline.\n",
        "\n",
        "Langchain-community library: For additional functionality and integrations.\n",
        "\n",
        "PyPDF library: For reading PDF documents.\n",
        "\n",
        "Accelerate library: For accelerating model loading.\n",
        "\n",
        "Bitsandbytes library: For 4-bit quantization.\n",
        "\n",
        "Library Import\n",
        "====================================\n",
        "The code imports the following libraries:\n",
        "\n",
        "os: For interacting with the operating system.\n",
        "\n",
        "torch: For working with PyTorch models and tensors.\n",
        "\n",
        "load_dataset: For loading datasets from Hugging Face.\n",
        "\n",
        "AutoTokenizer, BitsAndBytesConfig, and AutoModelForCausalLM: For loading pre-trained models and tokenizers.\n",
        "\n",
        "PyPDFLoader: For loading PDF documents.\n",
        "\n",
        "FAISS: For building a vector index.\n",
        "\n",
        "RecursiveCharacterTextSplitter: For splitting text into chunks.\n",
        "\n",
        "RetrievalQA: For building a retrieval-based QA system.\n",
        "\n",
        "PromptTemplate: For creating custom prompts.\n",
        "\n",
        "HuggingFaceEmbeddings: For generating vector embeddings.\n",
        "\n",
        "HuggingFaceHub: For loading models from the Hugging Face Hub.\n",
        "\n",
        "Hugging Face Hub Login\n",
        "The code logs in to the Hugging Face Hub using the notebook_login function.\n",
        "\n",
        "#Dataset Loading\n",
        "The code loads the \"case_hold\" subset of the \"lex_glue\" dataset using the load_dataset function."
      ],
      "metadata": {
        "id": "64PsIfKIB4Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the **dataset**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7g7zMSonvjK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. Data Loading\n",
        "# ==============================================================================\n",
        "# We'll use the 'lex_glue' dataset, specifically the 'case_hold' subset.\n",
        "#===============================================================================\n",
        "# 2. Load the Data Set\n",
        "#===============================================================================\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"lex_glue\", \"case_hold\")"
      ],
      "metadata": {
        "id": "13NRqfyAuOJE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "The code iterates over each document in the training split of the dataset and extracts string values. It appends these string values to a list called documents.\n",
        "\n",
        "Code\n",
        "\n",
        "Here's a step-by-step breakdown of the code:\n",
        "\n",
        "Initialize an empty list: documents = []\n",
        "Creates an empty list to store the preprocessed documents.\n",
        "\n",
        "Iterate over the dataset: for doc in dataset['train']\n",
        "Iterates over each document in the training split of the dataset.\n",
        "\n",
        "Extract string values: for key, value in doc.items(): if isinstance(value, str): documents.append(value)\n",
        "Iterates over each key-value pair in the document.\n",
        "Checks if the value is a string using the isinstance function.\n",
        "If the value is a string, appends it to the documents list.\n",
        "\n",
        "Print the number of documents: print(f\"Loaded {len(documents)} documents.\")\n",
        "Outputs the number of documents loaded.\n",
        "\n",
        "Output\n",
        "The code outputs the number of documents loaded, which is the total number of string values extracted from the dataset."
      ],
      "metadata": {
        "id": "F3VKMQx4DsqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================================\n",
        "# 3. Preprocess the data\n",
        "#===============================================================================\n",
        "# Preprocess the data\n",
        "\n",
        "documents = []\n",
        "for doc in dataset['train']:\n",
        "    for key, value in doc.items():\n",
        "        if isinstance(value, str):\n",
        "            documents.append(value)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWI5jAV_v709",
        "outputId": "ddda4a8d-6cee-4f44-b3f8-1b3b2cddf347"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 45000 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================================\n",
        "# Split data into Chunks\n",
        "#===============================================================================\n",
        "# ==============================================================================\n",
        "# 4. Split the Documents into Chunks\n",
        "# ==============================================================================\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunked_docs = []\n",
        "for doc in documents:\n",
        "    chunks = text_splitter.split_text(doc)\n",
        "    chunked_docs.extend(chunks)\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBNfGzg9wrGN",
        "outputId": "2032ec35-bc44-4f89-804b-9289597002bb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 45000 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now that you've created 45,000 chunks, the next step is to create vector embeddings for these chunks. Vector embeddings are dense vector representations of text that capture semantic meaning, allowing for efficient similarity search.#\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bYwOF5nAxYU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Vectorization\n",
        "\n",
        "Tokenization is the process of splitting text into individual words or tokens. Vectorization is the process of converting these tokens into numerical vectors that can be processed by machines.\n",
        "\n",
        "Why Not Tokenization and Vectorization?\n",
        "In this case, we're using a pre-trained model (all-MiniLM-L6-v2) that can directly generate vector embeddings for chunks of text without requiring explicit tokenization and vectorization steps. This approach simplifies the workflow and leverages the strengths of pre-trained models.\n",
        "\n",
        "# Vector Embeddings\n",
        "Vector embeddings are dense vector representations of text that capture semantic meaning. These vectors can be used for various natural language processing (NLP) tasks, such as text classification, clustering, and similarity search.\n",
        "\n",
        "How Vector Embeddings Address the Challenge\n",
        "===============================================================================\n",
        "Vector embeddings address the challenge of non-tokenized and non-vectorized text by:\n",
        "\n",
        "Capturing semantic meaning: Vector embeddings capture the semantic meaning of text, allowing for more accurate and nuanced representations.\n",
        "\n",
        "Handling variable-length text: Vector embeddings can handle variable-length text inputs, eliminating the need for explicit tokenization and padding.\n",
        "\n",
        "Enabling efficient similarity search: Vector embeddings enable efficient similarity search and retrieval, making it possible to quickly find relevant documents in large datasets.\n",
        "\n",
        "#Benefits of Using Pre-Trained Models\n",
        "Using pre-trained models like all-MiniLM-L6-v2 offers several benefits:\n",
        "Pre-trained on large datasets: These models are pre-trained on large datasets, allowing them to capture a wide range of semantic meanings and relationships.\n",
        "Efficient and scalable: Pre-trained models can be fine-tuned for specific tasks, making them efficient and scalable for various NLP applications.\n",
        "Simplified workflow: By leveraging pre-trained models, we can simplify the workflow and focus on higher-level tasks, such as indexing and retrieval.\n",
        "In summary, using vector embeddings generated by pre-trained models like all-MiniLM-L6-v2 offers a convenient and efficient way to capture semantic meaning and enable efficient similarity search and retrieval, without requiring explicit tokenization and vectorization steps.\n"
      ],
      "metadata": {
        "id": "m3poi73LFXot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Why Tokenization is Necessary\n",
        "Think of a computer as only understanding numbers, not words. You can't feed a legal contract, or even a single word, directly into a machine learning model.\n",
        "\n",
        "What it is: Tokenization is the process of breaking down raw text (like your document chunks) into smaller, manageable units called tokens. These tokens can be words (\"law\"), subwords (\"legal\", \"-ly\"), or even punctuation.\n",
        "\n",
        "Why we do it: The embedding model we use in the next step is designed to work with these tokens. The tokenizer provides a numerical \"ID\" for each token, which is the first step in converting human language into a format the computer can understand.\n",
        "\n",
        "2. Why Vectorization and Embeddings?\n",
        "This is the most powerful concept behind modern RAG systems.\n",
        "\n",
        "What it is: Vectorization is the process of converting your text chunks into numerical vectors (long lists of numbers) that capture the semantic meaning of the text. This is often called creating vector embeddings.\n",
        "\n",
        "Why we do it:\n",
        "\n",
        "Semantic Understanding: The magic of embeddings is that they map similar concepts to points that are close to each other in a high-dimensional space. For example, the vector for \"legal contract\" will be much closer to the vector for \"binding agreement\" than it is to \"space travel.\"\n",
        "\n",
        "The Power of Semantic Search: When a user asks a question like, \"What are the liabilities of the parties?\" the system doesn't just search for the keywords \"liabilities\" and \"parties.\" It converts the entire question into a vector and then searches the database for document chunks with a similar vector. This allows the system to retrieve highly relevant information, even if the phrasing is different. This is the \"Retrieval\" part of RAG.\n",
        "\n",
        "In summary, you take your text chunks, tokenize them, and then use an embedding model to convert those tokens into dense vectors. These vectors are then stored in a special database (like FAISS) that is optimized for finding similar vectors very quickly."
      ],
      "metadata": {
        "id": "GuojGXDDGpF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You did not need to write a separate line of code for tokenization because the HuggingFaceEmbeddings class, which is built on the sentence-transformers library, handles this automatically behind the scenes.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "When you create the embeddings object with HuggingFaceEmbeddings(model_name=\"...\"), it downloads not just the model weights, but also the pre-trained tokenizer associated with that specific model (all-MiniLM-L6-v2).\n",
        "\n",
        "When you call FAISS.from_texts(chunked_docs, embeddings), the embeddings object's internal encode() method is invoked.\n",
        "\n",
        "The encode() method takes your list of chunked_docs (which are just strings) and first sends them to the model's tokenizer.\n",
        "\n",
        "The tokenizer performs the conversion from text to numerical tokens.\n",
        "\n",
        "These numerical tokens are then fed into the all-MiniLM-L6-v2 model, which generates the final vector embeddings.\n",
        "\n",
        "So, the tokenization is seamlessly integrated into the process of creating the embeddings. This is a common practice in modern libraries to simplify the development pipeline, but it's great that you're questioning where that step happens!\n",
        "\n",
        "Since we have now successfully performed the vectorization, the next step is to set up the Large Language Model (LLM) for the \"Generation\" part of our RAG system."
      ],
      "metadata": {
        "id": "QIdsZvobG6A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create vector embeddings using the HuggingFaceEmbeddings class:"
      ],
      "metadata": {
        "id": "yD4LgGhnxuFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. Vector Embeddings and Indexing\n",
        "# ==============================================================================\n",
        "\n",
        "# Choose an embedding model. 'all-MiniLM-L6-v2' is a great, lightweight choice\n",
        "# known for its efficiency and strong performance in semantic search.\n",
        "# ==============================================================================\n",
        "# 5. Create Vector Embeddings\n",
        "# ==============================================================================\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"\\nLoading the embedding model...\")\n",
        "# Initialize the embedding model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Create vector embeddings for the chunks\n",
        "print(\"Creating vector embeddings...\")\n",
        "vector_embeddings = embeddings.embed_documents(chunked_docs)\n",
        "print(\"Vector embeddings created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6MjYSt9xAbs",
        "outputId": "4452a8ea-2e0b-4884-f289-2e811dbeae03"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the embedding model...\n",
            "Creating vector embeddings...\n",
            "Vector embeddings created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Steps\n",
        "\n",
        "With the vector embeddings created, you can proceed with further processing or analysis, such as:\n",
        "Indexing: Building an index of the vector embeddings for efficient similarity search and retrieval.\n",
        "Similarity search: Using the vector embeddings to find similar documents or chunks."
      ],
      "metadata": {
        "id": "zWPkFDO7Hz9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS Index\n",
        "The FAISS class provides an efficient way to build an index of vector embeddings, allowing for fast similarity search and retrieval. The from_texts method creates a FAISS index from the chunked documents and their corresponding vector embeddings.\n",
        "\n",
        "Benefits of FAISS Index\n",
        "=============================\n",
        "Using a FAISS index offers several benefits:\n",
        "Efficient similarity search: FAISS index enables fast and efficient similarity search, making it possible to quickly find relevant documents or chunks.\n",
        "Scalability: FAISS index can handle large datasets and scale to meet the needs of various applications."
      ],
      "metadata": {
        "id": "Jaewig_sIFTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. Build a Vector Index\n",
        "# ==============================================================================\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Build a FAISS index\n",
        "print(\"Building FAISS index...\")\n",
        "vector_store = FAISS.from_texts(chunked_docs, embeddings)\n",
        "print(\"FAISS index built.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT2qx6cqxyW1",
        "outputId": "018b9ba3-1281-45b8-dccc-9a18beab85a5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n",
            "FAISS index built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a FAISS (Facebook AI Similarity Search) index using the FAISS class from the langchain_community.vectorstores module."
      ],
      "metadata": {
        "id": "qvIrWGEXH7_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this code:\n",
        "# We initialize a FAISS index using the FAISS.from_texts method, passing in the chunked_docs and embeddings.\n",
        "# The FAISS.from_texts method creates a FAISS index that allows for efficient similarity search.\n",
        "# After building the vector index, you can use it to retrieve relevant chunks for a given query"
      ],
      "metadata": {
        "id": "pgwnhDJtx62P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've built a FAISS index, the next step is to use it for retrieval and potentially integrate it with a language model for generation. Here are a few options:\n",
        "# Similarity Search: You can use the FAISS index to perform similarity search and retrieve relevant chunks for a given query.\n",
        "# Retrieval-Augmented Generation (RAG): You can integrate the FAISS index with a language model to generate responses based on the retrieved chunks.\n"
      ],
      "metadata": {
        "id": "ygQXiABKyWu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "With the FAISS index built, you can proceed with further processing or analysis, such as:\n",
        "\n",
        "### Similarity search: Using the FAISS index to find similar documents or chunks.\n",
        "### Querying the index: Using a query string to search for relevant documents or chunks in the index.\n",
        "\n",
        "You can now use the vector_store object to perform similarity search and retrieval. For example, you can use the similarity_search method to find similar documents or chunks."
      ],
      "metadata": {
        "id": "Y3j2nuDtJMfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. Perform Similarity Search\n",
        "# ==============================================================================\n",
        "\n",
        "query = \"What are the key elements of a contract?\"\n",
        "docs = vector_store.similarity_search(query, k=5)\n",
        "\n",
        "print(\"Retrieved documents:\")\n",
        "for doc in docs:\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCW9KYmZyZ1d",
        "outputId": "bc4bea21-8cb8-4e6f-857f-1f7b5ac2130d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved documents:\n",
            "the contract is of a type usually put in writing, whether it needs a formal writing for its full expression, whether it has few or many details, whether the amount involved is large or small, whether it is a common or unusual contract, whether a standard form of contract is widely used in similar transactions, and whether either party takes any action in preparation for performance during the negotiations.” Horsfield Const., Inc. v. Dubuque County, Iowa, 653 N.W.2d 563 (Iowa 2002) (quoting Restatement (Seoond) op CONTRACTS § 27 cmt. c). b. Analysis of letter of intent The court first notes that the language of the letter of intent clearly contemplates a subsequent, final contract. Paragraph 9 of the letter of intent reads as follows: 9. Form of lting Group, Inc., 494 N.W.2d at 444-45 (<HOLDING>); Air Host Cedar Rapids, Inc. v. Cedar Rapids\n",
            "language approach to interpreting Section 97-36. This Court’s precedent identifies that a modified contract containing the required formation elements is a new contract. See, e.g., NRC Golf Course, LLC v. JMR Golf LLC,_N.C. App._,_, 731 S.E.2d 474, 480 (2012) (“Parties to a contract may agree to change its terms; but the new agreement, to be effective, must contain the elements necessary to the formation of a contract.” (emphasis added)). Like other newly formed contracts, a modified contract may be made in this state. The General Assembly crafted Section 97-36 with a full view that the phrase “employment contract” contemplated both contracts of hire as well as modifications of existing contracts which, by long-standing precedent, are new agreements. See id,.; co (Tex. App. 1983) (<HOLDING>). The Commission held that modification of an\n",
            "whether a broker is entitled to a commission for condemnation of property should be analyzed according to ordinary contract principles, including those applicable to oral contracts and contract modification. The focus should be on the agreed scope of the broker’s employment, not on extraneous factors such as when the state should take possession. We now discuss those principles and apply them to this case. B. Applying Contract Principles An oral contract, such as the one in this case, is subject to the basic requirements of contract law such as offer, acceptance, consideration and sufficient spe t a contract of employment between a broker and a principal may be oral or written); 12 Am.Jur.2d Brokers § 51 (1997) (same); cf. Futch v. Head, 511 So.2d 314, 317 (Fla. 1st DCA 1987) (<HOLDING>). Applying these principles to brokerage\n",
            "As such [t]he rules applicable to the construction of written contracts in general are to be applied in construing a post-nuptial agreement. Such a contract must be considered as a whole, and from such examination the intent of the parties must be gathered. Such construction should be given the agreement, if possible, as will render all of its clauses harmonious, so as to carry into effect the actual purpose and intent of the parties as derived therefrom. Roberts v. Roberts, 381 So.2d 1333, 1335 (Miss.1980). Property settlement agreements are contractual obligations arising on the date of final judgment of divorce and may be specifically enforced by a chancellor if such action is necessary to effectuate the terms of the agreement. See Mount v. Mount, 624 So.2d 1001, 1005 (Miss.1993) (<HOLDING>); Jones v. Jones, 532 So.2d 574, 580\n",
            "are “subject to the same contract construction principles that apply to any other species of contract.” Id. at 461. As this Court noted in Rohlman v Hawkeye-Security Ins Co, 442 Mich 520, 525 n 3; 502 NW2d 310 (1993), quoting 12A Couch, Insurance, 2d (rev ed), § 45:694, pp 331-332, [the insurance] policy and the statutes relating thereto must be read and construed together as though the statutes were a part of the contract, for it is to be presumed that the parties contracted with the intention of executing a policy satisfying the statutory requirements, and intended to make the contract to carry out its purpose. Thus, when a provision in an insurance policy is mandated by statute, the rights and limitations of the coverage are governed by that statute. See Rohlman, 442 Mich at 524-525 (<HOLDING>). On the other hand, when a provision in an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "We define a query and use the similarity_search method to retrieve the top 5 most similar chunks.\n",
        "We print the retrieved chunks.\n",
        "If you want to implement RAG, you'll need to integrate the FAISS index with a language model."
      ],
      "metadata": {
        "id": "VYEb9eONylCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity_search method is the core of the Retrieval component of your RAG system. It proves that your embeddings and FAISS index are working correctly by retrieving the most semantically relevant documents to your query.\n",
        "\n",
        "You've successfully built the first part of your RAG pipeline. The final steps are to integrate the LLM for the \"Generation\" part and then combine everything into a single, cohesive chain."
      ],
      "metadata": {
        "id": "tjZXqGGWJhVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Steps:\n",
        "\n",
        "With the similarity search results, you can proceed with further processing or analysis, such as:\n",
        "\n",
        "Summarizing the results: Summarizing the retrieved documents to provide a concise answer to the query.\n",
        "Further filtering: Further filtering the retrieved documents based on additional criteria, such as relevance or accuracy.\n",
        "\n",
        "we can also experiment with different queries and see how the similarity search results change.\n"
      ],
      "metadata": {
        "id": "AvEE2GQ3J2cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result shows that your Retrieval component is working perfectly. It has successfully taken your query, converted it into an embedding, and found the most semantically relevant legal document chunks from your FAISS index.\n",
        "\n",
        "Now, we move on to the final and most exciting part: Generation. This is where we will use a Large Language Model (LLM) to read the retrieved documents and generate a human-like, concise answer based on that information.\n",
        "\n",
        "This completes the full RAG pipeline."
      ],
      "metadata": {
        "id": "knyKM1AhKezl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before that going for summarisation"
      ],
      "metadata": {
        "id": "hKFWC0eoKw8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet uses the Hugging Face Transformers library to summarize the first retrieved document."
      ],
      "metadata": {
        "id": "ldVUVtHJK9y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the retrieved documents\n",
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "summary = summarizer(docs[0].page_content, max_length=100, min_length=5, do_sample=False)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6NimgIiK2Rd",
        "outputId": "653bbec9-6665-449c-fafd-44b49ebe49ee"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': ' The court first notes that the language of the letter of intent clearly contemplates a subsequent, final contract . Paragraph 9 of the contract reads as follows: 9. the contract is of a type usually put in writing, whether it needs a formal writing for its full expression .'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using API token from hugging face and LLM Initialisation:\n",
        "\n",
        "The below code block is a complete, step-by-step implementation of a Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "It's essentially showing an alternative way to build the final RAG chain that we were working on previously. Instead of using a high-level LangChain RetrievalQA chain, this code builds the pipeline manually, giving you more granular control."
      ],
      "metadata": {
        "id": "c2gyprbxMzft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indirect Method:"
      ],
      "metadata": {
        "id": "c27tZ0aaYVVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating API Token from hugging face**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qY7XaYTFNFjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create API Token\n",
        "#===============================================================================\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your token\" #token name from hugging face"
      ],
      "metadata": {
        "id": "JGLK3Eh2y9SM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Initialization**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LkCu6OUENkZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", huggingfacehub_api_token=\"your token\")"
      ],
      "metadata": {
        "id": "dmg1NjASzNHN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb13n8Bqzcxc",
        "outputId": "c3b9d1dc-9a32-4e59-d300-e3829770caa9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.68)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2025.6.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=\"your token\"\n",
        ")"
      ],
      "metadata": {
        "id": "LoRXKQbUzWRc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: This section is all about setting up the Large Language Model (LLM) you will use for the \"Generation\" part of RAG.\n",
        "================================================================================\n",
        "What it does: It shows two different ways to use a Hugging Face model:\n",
        "\n",
        "Environment Variable (os.environ): Sets your Hugging Face API token so other libraries can access it.\n",
        "\n",
        "LangChain API Access (HuggingFaceHub & HuggingFaceEndpoint): These classes from LangChain allow you to access a model on the Hugging Face Hub (like flan-t5-base) via its API. HuggingFaceEndpoint is a more recent and configurable way to do this."
      ],
      "metadata": {
        "id": "tjdcybroN1u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Manual RAG Implementation (Local Model Approach)\n",
        "\n",
        "The rest of the code is a detailed, manual recreation of a RAG pipeline. This is a great way to understand exactly what the high-level RetrievalQA chain was doing behind the scenes."
      ],
      "metadata": {
        "id": "eyD8gy-8OBHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're importing the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library. These classes are used to load pre-trained models and tokenizers."
      ],
      "metadata": {
        "id": "3Ygqr1ZJ6efd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load the Model and Tokenizer\n",
        "\n",
        "Step 2: Retrieve Relevant Documents\n",
        "\n",
        "Step 3: Create a Context String\n",
        "\n",
        "Step 4: Generate the Response:\n",
        "\n",
        "Purpose: This is the Generation step, where the LLM produces the final answer.\n",
        "\n",
        "What it does:\n",
        "\n",
        "Builds the Final Prompt: It creates a well-structured prompt string that includes both the context (the retrieved documents) and the original query.\n",
        "\n",
        "Tokenizes the Prompt: It uses the tokenizer to convert this combined prompt string into numerical tokens that the flan-t5 model can understand.\n",
        "\n",
        "Generates an Answer: It feeds these tokens into the model.generate() function to produce a response.\n",
        "\n",
        "Decodes the Output: It then uses the tokenizer to convert the model's numerical output back into a human-readable text string (response).\n",
        "\n",
        "In summary, this code block demonstrates a working RAG pipeline built from its fundamental components. It's a great way to understand the flow from retrieval to generation without relying on a single, high-level abstraction like the RetrievalQA chain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XI4mWM7ZOQFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 8. Generate Response\n",
        "# ==============================================================================\n",
        "# Importing the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library.\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Step 1: Load the Model and Tokenizer\n",
        "# We're loading a pre-trained model called flan-t5-base and its corresponding tokenizer.\n",
        "#===============================================================================\n",
        "# Load the model and tokenizer\n",
        "#===============================================================================\n",
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Purpose: Instead of using an API, this loads the actual flan-t5-base model and its tokenizer onto your local machine (or Colab GPU).\n",
        "# This gives you more control but requires more memory.\n",
        "\n",
        "## Step 2: Retrieve Relevant Documents\n",
        "#*******************************************************************************\n",
        "# We're defining a query about the key elements of a contract and using a vector_store to search for relevant documents.\n",
        "#===============================================================================\n",
        "# Retrieve relevant documents\n",
        "#===============================================================================\n",
        "query = \"What are the key elements of a contract?\"\n",
        "docs = vector_store.similarity_search(query, k=5)\n",
        "# Purpose: This is the Retrieval step.\n",
        "\n",
        "# What it does:\n",
        "# It takes a query, converts it into a vector, and then uses your vector_store (the FAISS index you built) to find the 5 most similar document chunks (k=5).\n",
        "\n",
        "#Step 3: Create a Context String\n",
        "#*******************************************************************************\n",
        "# We're creating a context string by concatenating the contents of the relevant documents.\n",
        "#===============================================================================\n",
        "# Create a context string\n",
        "#===============================================================================\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    context += doc.page_content + \"\\n\\n\"\n",
        "    if len(context) > 2000:  # truncate context to prevent excessive length\n",
        "        break\n",
        "# Purpose: To prepare the retrieved documents for the LLM.\n",
        "\n",
        "# What it does:\n",
        "# It loops through the retrieved documents (docs) and concatenates their text content (doc.page_content) into a single string called context. This is the information the LLM will use to answer the question. It also includes a safety check to prevent the context from getting too long.\n",
        "\n",
        "#Step 4: Generate the Response\n",
        "#*******************************************************************************\n",
        "# Generate response\n",
        "prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "output = model.generate(inputs[\"input_ids\"], max_new_tokens=512)\n",
        "\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzG2DSlb0RBE",
        "outputId": "f9da398f-ecca-4fe5-c26d-eaf259c52249"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "the contract itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: This is the Generation step, where the LLM produces the final answer.\n",
        "\n",
        "What it does:\n",
        "\n",
        "Builds the Final Prompt: It creates a well-structured prompt string that includes both the context (the retrieved documents) and the original query.\n",
        "\n",
        "Tokenizes the Prompt: It uses the tokenizer to convert this combined prompt string into numerical tokens that the flan-t5 model can understand.\n",
        "\n",
        "Generates an Answer: It feeds these tokens into the model.generate() function to produce a response.\n",
        "\n",
        "Decodes the Output: It then uses the tokenizer to convert the model's numerical output back into a human-readable text string (response).\n",
        "\n",
        "In summary, this code block demonstrates a working RAG pipeline built from its fundamental components. It's a great way to understand the flow from retrieval to generation without relying on a single, high-level abstraction like the RetrievalQA chain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xeSvopeyPnAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Retrieve relevant documents\n",
        "query = \"What are the key elements of a contract?\"\n",
        "docs = vector_store.similarity_search(query, k=5)\n",
        "\n",
        "# Create a context string\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    context += doc.page_content + \"\\n\\n\"\n",
        "    if len(context) > 2000:  # truncate context to prevent excessive length\n",
        "        break\n",
        "\n",
        "# Generate response\n",
        "prompt = f\"Based on general knowledge of contracts, answer the following question: {query}. Provide a detailed response.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "output = model.generate(inputs[\"input_ids\"], max_new_tokens=1024, num_beams=4)\n",
        "\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qaDiuEt0ag1",
        "outputId": "113a72b5-0c9d-4f99-b6a9-037bf055c1ed"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "A contract is a contract between a party to a contract and a party to a contract.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This not a good response .so trying further."
      ],
      "metadata": {
        "id": "GWfROAiVZ_Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "generator = pipeline('text-generation', model='gpt2-medium')\n",
        "\n",
        "\n",
        "# We're defining a query about the key elements of a contract and using a vector_store to search for relevant documents.\n",
        "#===============================================================================\n",
        "# Retrieve relevant documents\n",
        "#===============================================================================\n",
        "query = \"What are the key elements of a contract?\"\n",
        "docs = vector_store.similarity_search(query, k=5)\n",
        "\n",
        "\n",
        "#  We're creating a context string by concatenating the contents of the relevant documents.\n",
        "#===============================================================================\n",
        "# Create a context string\n",
        "#===============================================================================\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    context += doc.page_content + \"\\n\\n\"\n",
        "    if len(context) > 2000:  # truncate context to prevent excessive length\n",
        "        break\n",
        "\n",
        "# We're defining a prompt and using the tokenizer to convert it into input IDs. We're then generating output using the model.\n",
        "#================================================================================\n",
        "# Generate response\n",
        "#===============================================================================\n",
        "prompt = f\"What are the key elements of a contract? Consider offer, acceptance, consideration, and other essential components. Provide a detailed response.\"\n",
        "response = generator(prompt, max_length=1024, num_return_sequences=1)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGIP7fgu0Q-E",
        "outputId": "cdae2e7c-be11-4518-c54a-3d9f2368f706"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "What are the key elements of a contract? Consider offer, acceptance, consideration, and other essential components. Provide a detailed response.\n",
            "\n",
            "What if I need help understanding a contract? Here are some suggestions:\n",
            "\n",
            "Read the contract carefully to see if it includes all of the steps required for the transaction. If not, try to understand the terms of the contract.\n",
            "\n",
            "Watch the contract carefully. If a contract is unclear, or if you may need more information, go to the appropriate step and contact Mr. or Mrs. E. If you aren't sure what the rights are, ask for clarification.\n",
            "\n",
            "Ask the contract's author to respond to your questions, if possible. The author's response may be included in the contract.\n",
            "\n",
            "Be aware that contract terms vary from state to state.\n",
            "\n",
            "What should I do if I believe that my rights have been violated?\n",
            "\n",
            "If you believe that your rights have been violated, you should seek redress through the appropriate state and national courts. You may also wish to contact a representative of the contract's author.\n",
            "\n",
            "What if I don't want to hire the contract's author?\n",
            "\n",
            "If you are not sure who to contact, you can contact a representative of the contract's author. You may also wish to contact a representative of the contract's author.\n",
            "\n",
            "What if I am the sole contract author or no contract author\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated response provides some useful information about contracts, but it seems to be a bit disjointed and doesn't directly address the key elements of a contract in a clear and concise manner.\n",
        "Let's try to refine the prompt and see if we can get a more focused response:"
      ],
      "metadata": {
        "id": "gNOK9HhL1iI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "generator = pipeline('text-generation', model='gpt2-medium')\n",
        "\n",
        "# Generate response\n",
        "prompt = \"The key elements of a contract include offer, acceptance, and consideration. Other essential components are\"\n",
        "response = generator(prompt, max_length=256, num_return_sequences=1)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Uj97dp1che",
        "outputId": "ef42ac74-562a-40c1-ca0c-65346713a63c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "The key elements of a contract include offer, acceptance, and consideration. Other essential components are:\n",
            "\n",
            "The offer. The offer must be reasonable, and should include enough time to allow the buyer to review all relevant details. This may include:\n",
            "\n",
            "The price, which should be reasonable\n",
            "\n",
            "The type of property, if any, in the purchase – for example, a house or a housebuilding company\n",
            "\n",
            "The time, if it takes – for example, the time it takes to make the decision to sell\n",
            "\n",
            "If any additional terms are added to the offer, and/or if the buyer wishes to pay for the additional services\n",
            "\n",
            "The consideration. The buyer must be able to accept the proposed offering.\n",
            "\n",
            "What is the purpose of the contract?\n",
            "\n",
            "The buyer's purpose for entering into the contract is to determine whether the price the buyer is willing to pay is reasonable and if so, how much that is. They are also likely to make an informed decision about the purchase price, and should be aware of any additional costs involved.\n",
            "\n",
            "What are the terms of the contract?\n",
            "\n",
            "At the heart of the contract is a reasonable price for the property. Any additional cost, including commission and legal fees, should be covered by the buyer.\n",
            "\n",
            "The buyer must also be able to accept and understand all of the following:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated response provides some useful information about contracts, but it seems to be repetitive and doesn't provide a clear and concise overview of the key elements of a contract.\n",
        "Let's try to refine the prompt and model parameters to get a better response:"
      ],
      "metadata": {
        "id": "nOaUR_YV1t3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "generator = pipeline('text-generation', model='gpt2-medium', max_length=512)\n",
        "\n",
        "# Generate response\n",
        "prompt = \"The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail.\"\n",
        "response = generator(prompt, num_return_sequences=1, truncation=True)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEpIQ1DZ1vn9",
        "outputId": "0f55ce4b-cd99-4c87-8625-a22eddac0980"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail.\n",
            "\n",
            "2. Describe the purpose of the contract and the relationship between the parties. Explain how it will effect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated response seems to be cut off and doesn't provide a detailed explanation of the key elements of a contract.\n",
        "Let's try to adjust the model parameters to generate a longer and more comprehensive response:"
      ],
      "metadata": {
        "id": "LdZRS7IU2DhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this code:\n",
        "We're setting device=-1 to load the model on the CPU instead of the CUDA device.\n",
        "# This should prevent the CUDA error and allow the model to generate a response."
      ],
      "metadata": {
        "id": "PiJ5ENg43RHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code performs direct text generation using a pre-trained language model from the Hugging Face transformers library.\n",
        "\n",
        "Unlike the RAG pipeline we were building, this code does not retrieve any information from an external document base. Instead, it relies solely on the internal knowledge of the model itself to generate a response to the prompt."
      ],
      "metadata": {
        "id": "DORv_ImJQgfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model and Pipeline Loading-\n",
        "\n",
        "from transformers import pipeline: This imports the high-level pipeline function, which is a convenient way to use pre-trained models for specific tasks without needing to manually handle tokenization, model inference, and output processing.\n",
        "\n",
        "generator = pipeline(...): This line initializes a text generation pipeline.\n",
        "\n",
        "'text-generation' specifies the task you want to perform.\n",
        "\n",
        "model='gpt2-medium' tells the pipeline to use the gpt2-medium model from the Hugging Face Hub. It will automatically download the model weights and its associated tokenizer.\n",
        "\n",
        "device=-1 specifies that the model should be loaded and run on the CPU. If you had a GPU available, you would use device=0 (or device=1, etc.) to use it for faster performance.\n",
        "# 2. Defining the Prompt and Generating Response\n",
        "\n",
        "prompt = \"...\": This defines the initial text that the model will use as a starting point. The model will try to continue this text in a coherent and relevant way.\n",
        "\n",
        "response = generator(...): This is the core command that generates the text.\n",
        "\n",
        "The first argument, prompt, is the input text.\n",
        "\n",
        "max_new_tokens=512 sets the maximum number of new tokens (words or subwords) that the model should generate.\n",
        "\n",
        "num_return_sequences=1 specifies that the model should generate only one output text sequence. You could increase this to, for example, 3 to get three different generated responses for the same prompt.\n",
        "\n",
        "# 3. Printing the Response\n",
        "\n",
        "print(response[0]['generated_text']): The generator() function returns a list of dictionaries. Each dictionary contains the generated text under the key 'generated_text'. This line accesses the first (and in this case, only) dictionary in the list and prints the generated text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DBIeiZ4uQrm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 1. Model and Pipeline Loading\n",
        "# We're loading a pre-trained model called GPT-2 MEDIUM and its corresponding tokenizer.\n",
        "#===============================================================================\n",
        "# Load the model and tokenizer\n",
        "#===============================================================================\n",
        "generator = pipeline('text-generation', model='gpt2-medium', device=-1)\n",
        "# This line loads a pre-trained GPT-2 model with the text-generation pipeline. The device=-1 parameter specifies that the model should be loaded on the CPU (instead of a GPU).\n",
        "\n",
        "# 2. Defining the Prompt and Generating Response\n",
        "#This line defines the prompt that will be used to generate text.\n",
        "#===============================================================================\n",
        "# Defining the Prompt and Generating Response:\n",
        "#===============================================================================\n",
        "prompt = \"The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail.\"\n",
        "response = generator(prompt, max_new_tokens=512, num_return_sequences=1) # This line generates text based on the prompt. The max_new_tokens=512 parameter specifies the maximum number of new tokens to generate, and num_return_sequences=1 specifies that only one sequence of text should be generated.\n",
        "\n",
        "# 3. Printing the Response\n",
        "# Printing the response:\n",
        "#================================================================================\n",
        "print(\"Generated response:\")\n",
        "print(response[0]['generated_text'])\n",
        "# This line prints the generated text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mtnAZ5r2UTk",
        "outputId": "27641d24-5f2c-4bb6-da53-0174d40a31da"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail.\n",
            "\n",
            "A contract is a legal instrument that sets out the terms and conditions of a legal relationship. The terms of a contract, as well as the conditions for acceptance, are the legal provisions that are binding on the parties. They are the laws that we agree to when we enter into a contract. A contract is an agreement between two or more parties that describes a legal relationship between them.\n",
            "\n",
            "The law of contracts defines the legal relationship between the parties. Some terms of a contract may be defined by statute or by judicial decision. For example, courts may define a term of a contract to include the term \"sale or rental of land, interest in land, interest in all or any part of land, or a right to use any land.\"\n",
            "\n",
            "\n",
            "The legal relationship between the buyer and the seller is commonly called the \"contract of sale.\" The terms of the contract are the legal terms that will govern the terms of the transaction. By way of example, a contract of sale is a contract between two or more parties to which an obligation is set out.\n",
            "\n",
            "A legal agreement may be defined by the law of the country where the contract is to be executed. For example, when a client hires a lawyer for representation in a criminal matter, the lawyer will have the legal authority to make the legal agreement.\n",
            "\n",
            "A contract may be defined by the law of the country where the contract is to be executed. For example, when a client hires a lawyer for representation in a criminal matter, the lawyer will have the legal authority to make the legal agreement.\n",
            "\n",
            "The legal authorities of two or more countries may agree to define terms or conditions that relate to a legal relationship between them. For example, when the United States and Mexico sign a trade agreement, they may agree on terms that govern the legal relationship between the United States and Mexico. The trade agreement may include a legal agreement between the United States and Mexico that provides for the provision of services to Mexico.\n",
            "\n",
            "The legal authorities of two or more countries may agree to define terms or conditions that relate to a legal relationship between them. For example, when the United States and Mexico sign a trade agreement, they may agree on terms that govern the legal relationship between the United States and Mexico. The trade agreement may include a legal agreement between the United States and Mexico that provides for the provision of services to Mexico.\n",
            "\n",
            "A legal agreement may be defined by the law of the country in which the legal relationship is to be executed. For example, when two states enter into a treaty, they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated response provides a detailed and comprehensive overview of contract essentials, including payment terms, provisions for withdrawal, and settlement negotiations.\n",
        "It seems like the model was able to generate a lengthy and coherent response that covers various aspects of contracts."
      ],
      "metadata": {
        "id": "-ZTA_Fph3DAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Details\n",
        "==============\n",
        "Model Name: GPT-2 Medium\n",
        "Model Type: Transformer-based language model\n",
        "Use Case: Text generation\n",
        "\n",
        "Parameters\n",
        "====================\n",
        "max_new_tokens: The maximum number of new tokens to generate. In this case, it's set to 512.\n",
        "num_return_sequences: The number of sequences to generate. In this case, it's set to 1.\n",
        "\n",
        "Output\n",
        "====================\n",
        "The code will output the generated text based on the prompt. The text will explain the key elements of a contract, including offer, acceptance, and consideration, as well as other essential parts of a contract."
      ],
      "metadata": {
        "id": "FGfB98Ak3C-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==============================================================================\n",
        "# LLM Setup (for the Generation step of RAG)\n",
        "# ==============================================================================\n",
        "\n",
        "For this step, we will use a small, open-source LLM that can run on Colab.\n",
        "A great choice is a quantized version of a model from the Hugging Face Hub\n",
        "to save GPU memory. Here, we'll use a version of 'mistralai/Mistral-7B-Instruct-v0.2'.\n",
        "Make sure you have a Hugging Face Hub token set up in Colab for access.\n",
        "If you don't have a token, you can also use `LlamaCpp` or a simple QA model,\n",
        "but a chat model gives better conversational results.\n",
        "\n",
        "# To set up your token: Go to Hugging Face, create an account, get an access token,\n",
        "# and add it in Colab Secrets or a prompt.\n",
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n"
      ],
      "metadata": {
        "id": "x4r43cVeVNq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Direct method:**Using RetrievalQA\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mTOAEbWqV7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 8.LLM Setup (for the Generation step of RAG)\n",
        "# ==============================================================================\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# We are using an openly accessible model.\n",
        "llm_model_name = \"google/flan-t5-base\"\n",
        "print(f\"\\nLoading the LLM: {llm_model_name}\")\n",
        "\n",
        "# Create a transformers pipeline for text-generation\n",
        "# We pass in the model and tokenizer to the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=AutoModelForSeq2SeqLM.from_pretrained(llm_model_name),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(llm_model_name),\n",
        "    max_length=512,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Now, wrap the transformers pipeline in the HuggingFacePipeline class.\n",
        "# This makes it compatible with LangChain's RetrievalQA.\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"LLM loaded and ready.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. Building the RAG Chain and Querying\n",
        "# ==============================================================================\n",
        "\n",
        "# A custom prompt helps guide the LLM to provide a relevant, concise answer.\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----------------\n",
        "{context}\n",
        "----------------\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Create a prompt instance with the template.\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Initialize the RAG chain. This chain orchestrates the entire RAG process.\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 10. Example Queries\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- RAG System Ready ---\")\n",
        "print(\"Enter your questions about the legal documents. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"\\nYour question: \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Exiting. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"Searching for answer...\")\n",
        "\n",
        "    response = rag_chain.invoke(user_query)\n",
        "\n",
        "    print(f\"\\nAI Response: {response['result']}\")"
      ],
      "metadata": {
        "id": "9Plor9ox2UK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8bdad1-6ecd-452e-a86d-10731f471b00"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the LLM: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM loaded and ready.\n",
            "\n",
            "--- RAG System Ready ---\n",
            "Enter your questions about the legal documents. Type 'exit' to quit.\n",
            "\n",
            "Your question: What is the difference between an 'offer' and an 'invitation to treat'?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (964 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for answer...\n",
            "\n",
            "AI Response: An offer' is \"the manifestation of willingness to enter into a bargain, so made as to justify another person in understanding that his assent to that bargain is invited and will conclude it.\"\n",
            "\n",
            "Your question: Explain the concept of 'force majeure' in a contract.\n",
            "Searching for answer...\n",
            "\n",
            "AI Response: force majeure clause is not intended to ... is one that is inferred from the statements or conduct of the parties. It is not a promise defined by the law, but one made by the parties, though not expressly.”\n",
            "\n",
            "Your question: What are the key elements of a legally binding contract?\n",
            "Searching for answer...\n",
            "\n",
            "AI Response: offer, acceptance, consideration and sufficient spe t a contract of employment between a broker and a principal may be oral or written\n",
            "\n",
            "Your question: exit\n",
            "Exiting. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. Building the RAG Chain and Querying\n",
        "# ==============================================================================\n",
        "\n",
        "# A custom prompt helps guide the LLM to provide a relevant, concise answer.\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----------------\n",
        "{context}\n",
        "----------------\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Create a prompt instance with the template.\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Initialize the RAG chain. This chain orchestrates the entire RAG process.\n",
        "# We will now pass search_kwargs to the retriever to retrieve fewer documents.\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # Pass search_kwargs to the retriever to only get the top 1 most relevant document.\n",
        "    # This helps avoid retrieving irrelevant information.\n",
        "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 1}),\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Example Queries\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- RAG System Ready ---\")\n",
        "print(\"Enter your questions about the legal documents. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"\\nYour question: \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Exiting. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"Searching for answer...\")\n",
        "\n",
        "    response = rag_chain.invoke(user_query)\n",
        "\n",
        "    print(f\"\\nAI Response: {response['result']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N23QtLUjUI--",
        "outputId": "4db2994e-3ca1-4a36-8b29-b946ba880543"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RAG System Ready ---\n",
            "Enter your questions about the legal documents. Type 'exit' to quit.\n",
            "\n",
            "Your question: What are the key elements of a legally binding contract?\n",
            "Searching for answer...\n",
            "\n",
            "AI Response: offer, acceptance, consideration and sufficient spe t a contract of employment between a broker and a principal may be oral or written; 12 Am.Jur.2d Brokers  51 (1997) (same); cf. Futch v. Head, 511 So.2d 314, 317 (Fla. 1st DCA 1987) (HOLDING>).\n",
            "\n",
            "Your question: Explain the concept of 'force majeure' in a contract.\n",
            "Searching for answer...\n",
            "\n",
            "AI Response: force majeure clause is inconsistent with the absence of any discussions between the parties indicating the common understanding of a force-majeure clause was not intended by the parties and with the purpose of a pr 06, 908 (Iowa 1977) (HOLDING>).\n",
            "\n",
            "Your question: What is the difference between an 'offer' and an 'invitation to treat'?\n",
            "Searching for answer...\n",
            "\n",
            "AI Response: An offer' is \"the manifestation of willingness to enter into a bargain, so made as to justify another person in understanding that his assent to that bargain is invited and will conclude it.\" Restatement (Second) Contracts (“Restatement”)  24 (1981). Thus, as the court recognized, the acceptance of an offer may be manifested by \"assent to the terms thereof made by the offeree in a manner invited or required by the offer.\" American Capital I, 58 Fed.Cl. at 407 (citing Restatement  50(1)). Since the August 29, 1986 Assistance Agreement incorporated \"a resolution or action of the [FHLBB] approving, or adopted concurrently with, this Agreement,\" it manifested acceptance of the offer contained in FHLBB Resolution 86-864. See Winstar, 518 U.S. at 890, 116 S.Ct. 2432 (HOLDING>).\n",
            "\n",
            "Your question: exit\n",
            "Exiting. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new output is much better! It shows that the fix of setting k=1 worked perfectly. The system is no longer combining two different document chunks. The model has correctly retrieved a single, relevant piece of information.\n",
        "\n",
        "What is Happening Now\n",
        "The new problem is with the Generation step. The flan-t5-base model is designed for a variety of tasks, but it is not the best at instruction-following or complex summarization. It is essentially taking the document snippet and re-writing it, but it isn't truly \"explaining\" the concept. The model is too \"passive.\"\n",
        "\n",
        "How to Fix It\n",
        "This is a classic case of needing better prompt engineering or a more capable LLM. We'll try to solve this with a simple prompt change first.\n",
        "\n",
        "The current prompt asks the model to \"answer the user's question,\" which can be interpreted by a model like Flan-T5 as simply reproducing the context."
      ],
      "metadata": {
        "id": "kF9kCe5lUgyF"
      }
    }
  ]
}