# -*- coding: utf-8 -*-
"""RAG project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hOWaa2b8rTc7TGaVmijNYDHKyf29ALgB

# Project: RAG System for Legal Document Q&A and Clause Retrieval
================================================================================
# Author: simmy xavier
# GitHub: github.com/siim2mary
#
#Project Overview
#===============================================================================
The project aims to build a Retrieval-Augmented Generation (RAG) system for answering questions about legal documents. The system uses a combination of natural language processing (NLP) and information retrieval techniques to:
Retrieve relevant text chunks: The system searches for relevant text chunks in a dataset of contracts.
Generate answers: The system uses a Large Language Model (LLM) to generate answers to questions based on the retrieved text chunks.

#Project Components:
#===============================================================================
The project covers the following components:

1. Loading a specialized dataset: The project uses a dataset of contracts from Hugging Face.
2. Creating text chunks: The project creates text chunks from the contracts dataset for retrieval.
3. Generating vector embeddings: The project generates vector embeddings for the text chunks using a sentence transformer model.
4. Building a vector index: The project builds a vector index using FAISS (Facebook AI Similarity Search) for efficient similarity search.
5. Implementing a RAG pipeline: The project implements a simple RAG pipeline to answer questions about legal documents.

#Goals and Objectives:
#===============================================================================

The goal of the project is to create a system that can accurately answer questions about legal documents by leveraging the strengths of both information retrieval and language generation.

Technical Requirements:
==============================================================================
The project requires the following technical components:
1. Datasets library: To load the contracts dataset from Hugging Face.
2. Sentence transformers library: To generate vector embeddings for the text chunks.
3. FAISS library: To build a vector index for efficient similarity search.
4. Langchain library: To manage the RAG pipeline.
5. PyPDF library: To read PDF documents (optional).

Overall, the project aims to demonstrate how to build a RAG system for answering questions about legal documents using open-source libraries.

# **What is RAG?**
RAG stands for Retrieval-Augmented Generation. It's a technique used in natural language processing (NLP) to generate text based on a given prompt or query. RAG combines the strengths of two approaches:
Retrieval: This involves searching for relevant information in a large database or knowledge base.
Generation: This involves generating text based on the retrieved information.

# Why is RAG used?
RAG is used to generate more accurate and informative text by leveraging the knowledge stored in a database or knowledge base. It's particularly useful for tasks like:

## Question answering: RAG can retrieve relevant information and generate answers to questions.
## Text summarization: RAG can retrieve relevant information and generate summaries of text.
## Text generation: RAG can retrieve relevant information and generate text based on a prompt or query.

What have we done so far?
We've been working on a project that uses RAG to generate text about contracts. Specifically, we've been trying to generate responses to questions about the key elements of a contract.

# Code Explanation
Let's go through the code we've written so far:
"""

print("Installing required libraries...")
!pip install -q datasets sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes

!pip install -U langchain-community

!pip install -q datasets sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes

!pip install -q datasets sentence-transformers faiss-cpu langchain langchain-community pypdf accelerate bitsandbytes!pip install -q datasets sentence-transformers faiss-cpu langchain langchain-community pypdf accelerate bitsandbytes

"""#Import Libraries

---


"""

# Import all necessary libraries.
import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub

print("Setup complete.")

from langchain.text_splitter import RecursiveCharacterTextSplitter

"""
The RecursiveCharacterTextSplitter is used to split a text into chunks of up to 1000 characters, with an overlap of 200 characters between consecutive chunks. The separators parameter specifies that the text should be split at newline characters, spaces, or empty strings.
Use Case
The RecursiveCharacterTextSplitter is particularly useful in natural language processing (NLP) applications, such as:
Text summarization: Breaking down large documents into smaller chunks for summarization.
Question answering: Splitting text into chunks to improve the accuracy of question answering models.
Information retrieval: Indexing and retrieving specific chunks of text from large documents.
By using the RecursiveCharacterTextSplitter, you can efficiently process and analyze large texts, making it easier to extract insights and meaning from the data.


"""

from huggingface_hub import notebook_login
notebook_login()

!pip install --upgrade datasets

"""#what we have done so far?
Installs the necessary libraries, imports them, and loads a dataset.

Library Installation
=============================
The code installs the following libraries:
Datasets library: For loading datasets from Hugging Face.

Sentence transformers library: For generating vector embeddings.

FAISS library: For building a vector index for efficient similarity search.

Langchain library: For managing the RAG pipeline.

Langchain-community library: For additional functionality and integrations.

PyPDF library: For reading PDF documents.

Accelerate library: For accelerating model loading.

Bitsandbytes library: For 4-bit quantization.

Library Import
====================================
The code imports the following libraries:

os: For interacting with the operating system.

torch: For working with PyTorch models and tensors.

load_dataset: For loading datasets from Hugging Face.

AutoTokenizer, BitsAndBytesConfig, and AutoModelForCausalLM: For loading pre-trained models and tokenizers.

PyPDFLoader: For loading PDF documents.

FAISS: For building a vector index.

RecursiveCharacterTextSplitter: For splitting text into chunks.

RetrievalQA: For building a retrieval-based QA system.

PromptTemplate: For creating custom prompts.

HuggingFaceEmbeddings: For generating vector embeddings.

HuggingFaceHub: For loading models from the Hugging Face Hub.

Hugging Face Hub Login
The code logs in to the Hugging Face Hub using the notebook_login function.

#Dataset Loading
The code loads the "case_hold" subset of the "lex_glue" dataset using the load_dataset function.

## Load the **dataset**

---
"""

# ==============================================================================
# 2. Data Loading
# ==============================================================================
# We'll use the 'lex_glue' dataset, specifically the 'case_hold' subset.
#===============================================================================
# 2. Load the Data Set
#===============================================================================
from datasets import load_dataset

dataset = load_dataset("lex_glue", "case_hold")

"""# Data Preprocessing
The code iterates over each document in the training split of the dataset and extracts string values. It appends these string values to a list called documents.

Code

Here's a step-by-step breakdown of the code:

Initialize an empty list: documents = []
Creates an empty list to store the preprocessed documents.

Iterate over the dataset: for doc in dataset['train']
Iterates over each document in the training split of the dataset.

Extract string values: for key, value in doc.items(): if isinstance(value, str): documents.append(value)
Iterates over each key-value pair in the document.
Checks if the value is a string using the isinstance function.
If the value is a string, appends it to the documents list.

Print the number of documents: print(f"Loaded {len(documents)} documents.")
Outputs the number of documents loaded.

Output
The code outputs the number of documents loaded, which is the total number of string values extracted from the dataset.
"""

#===============================================================================
# 3. Preprocess the data
#===============================================================================
# Preprocess the data

documents = []
for doc in dataset['train']:
    for key, value in doc.items():
        if isinstance(value, str):
            documents.append(value)

print(f"Loaded {len(documents)} documents.")

#===============================================================================
# Split data into Chunks
#===============================================================================
# ==============================================================================
# 4. Split the Documents into Chunks
# ==============================================================================

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Initialize the text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=150,
    separators=["\n\n", "\n", ".", "?", "!", " ", ""]
)

# Split the documents into chunks
chunked_docs = []
for doc in documents:
    chunks = text_splitter.split_text(doc)
    chunked_docs.extend(chunks)

print(f"Created {len(chunked_docs)} chunks.")

"""#Now that you've created 45,000 chunks, the next step is to create vector embeddings for these chunks. Vector embeddings are dense vector representations of text that capture semantic meaning, allowing for efficient similarity search.#

Tokenization and Vectorization

Tokenization is the process of splitting text into individual words or tokens. Vectorization is the process of converting these tokens into numerical vectors that can be processed by machines.

Why Not Tokenization and Vectorization?
In this case, we're using a pre-trained model (all-MiniLM-L6-v2) that can directly generate vector embeddings for chunks of text without requiring explicit tokenization and vectorization steps. This approach simplifies the workflow and leverages the strengths of pre-trained models.

# Vector Embeddings
Vector embeddings are dense vector representations of text that capture semantic meaning. These vectors can be used for various natural language processing (NLP) tasks, such as text classification, clustering, and similarity search.

How Vector Embeddings Address the Challenge
===============================================================================
Vector embeddings address the challenge of non-tokenized and non-vectorized text by:

Capturing semantic meaning: Vector embeddings capture the semantic meaning of text, allowing for more accurate and nuanced representations.

Handling variable-length text: Vector embeddings can handle variable-length text inputs, eliminating the need for explicit tokenization and padding.

Enabling efficient similarity search: Vector embeddings enable efficient similarity search and retrieval, making it possible to quickly find relevant documents in large datasets.

#Benefits of Using Pre-Trained Models
Using pre-trained models like all-MiniLM-L6-v2 offers several benefits:
Pre-trained on large datasets: These models are pre-trained on large datasets, allowing them to capture a wide range of semantic meanings and relationships.
Efficient and scalable: Pre-trained models can be fine-tuned for specific tasks, making them efficient and scalable for various NLP applications.
Simplified workflow: By leveraging pre-trained models, we can simplify the workflow and focus on higher-level tasks, such as indexing and retrieval.
In summary, using vector embeddings generated by pre-trained models like all-MiniLM-L6-v2 offers a convenient and efficient way to capture semantic meaning and enable efficient similarity search and retrieval, without requiring explicit tokenization and vectorization steps.

Why Tokenization is Necessary
Think of a computer as only understanding numbers, not words. You can't feed a legal contract, or even a single word, directly into a machine learning model.

What it is: Tokenization is the process of breaking down raw text (like your document chunks) into smaller, manageable units called tokens. These tokens can be words ("law"), subwords ("legal", "-ly"), or even punctuation.

Why we do it: The embedding model we use in the next step is designed to work with these tokens. The tokenizer provides a numerical "ID" for each token, which is the first step in converting human language into a format the computer can understand.

2. Why Vectorization and Embeddings?
This is the most powerful concept behind modern RAG systems.

What it is: Vectorization is the process of converting your text chunks into numerical vectors (long lists of numbers) that capture the semantic meaning of the text. This is often called creating vector embeddings.

Why we do it:

Semantic Understanding: The magic of embeddings is that they map similar concepts to points that are close to each other in a high-dimensional space. For example, the vector for "legal contract" will be much closer to the vector for "binding agreement" than it is to "space travel."

The Power of Semantic Search: When a user asks a question like, "What are the liabilities of the parties?" the system doesn't just search for the keywords "liabilities" and "parties." It converts the entire question into a vector and then searches the database for document chunks with a similar vector. This allows the system to retrieve highly relevant information, even if the phrasing is different. This is the "Retrieval" part of RAG.

In summary, you take your text chunks, tokenize them, and then use an embedding model to convert those tokens into dense vectors. These vectors are then stored in a special database (like FAISS) that is optimized for finding similar vectors very quickly.

You did not need to write a separate line of code for tokenization because the HuggingFaceEmbeddings class, which is built on the sentence-transformers library, handles this automatically behind the scenes.

Here's how it works:

When you create the embeddings object with HuggingFaceEmbeddings(model_name="..."), it downloads not just the model weights, but also the pre-trained tokenizer associated with that specific model (all-MiniLM-L6-v2).

When you call FAISS.from_texts(chunked_docs, embeddings), the embeddings object's internal encode() method is invoked.

The encode() method takes your list of chunked_docs (which are just strings) and first sends them to the model's tokenizer.

The tokenizer performs the conversion from text to numerical tokens.

These numerical tokens are then fed into the all-MiniLM-L6-v2 model, which generates the final vector embeddings.

So, the tokenization is seamlessly integrated into the process of creating the embeddings. This is a common practice in modern libraries to simplify the development pipeline, but it's great that you're questioning where that step happens!

Since we have now successfully performed the vectorization, the next step is to set up the Large Language Model (LLM) for the "Generation" part of our RAG system.

# Create vector embeddings using the HuggingFaceEmbeddings class:
"""

# ==============================================================================
# 5. Vector Embeddings and Indexing
# ==============================================================================

# Choose an embedding model. 'all-MiniLM-L6-v2' is a great, lightweight choice
# known for its efficiency and strong performance in semantic search.
# ==============================================================================
# 5. Create Vector Embeddings
# ==============================================================================

from langchain_community.embeddings import HuggingFaceEmbeddings

print("\nLoading the embedding model...")
# Initialize the embedding model
model_name = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=model_name)

# Create vector embeddings for the chunks
print("Creating vector embeddings...")
vector_embeddings = embeddings.embed_documents(chunked_docs)
print("Vector embeddings created.")

"""Next Steps

With the vector embeddings created, you can proceed with further processing or analysis, such as:
Indexing: Building an index of the vector embeddings for efficient similarity search and retrieval.
Similarity search: Using the vector embeddings to find similar documents or chunks.

# FAISS Index
The FAISS class provides an efficient way to build an index of vector embeddings, allowing for fast similarity search and retrieval. The from_texts method creates a FAISS index from the chunked documents and their corresponding vector embeddings.

Benefits of FAISS Index
=============================
Using a FAISS index offers several benefits:
Efficient similarity search: FAISS index enables fast and efficient similarity search, making it possible to quickly find relevant documents or chunks.
Scalability: FAISS index can handle large datasets and scale to meet the needs of various applications.
"""

# ==============================================================================
# 6. Build a Vector Index
# ==============================================================================

from langchain_community.vectorstores import FAISS

# Build a FAISS index
print("Building FAISS index...")
vector_store = FAISS.from_texts(chunked_docs, embeddings)
print("FAISS index built.")

"""This code builds a FAISS (Facebook AI Similarity Search) index using the FAISS class from the langchain_community.vectorstores module.

# In this code:
# We initialize a FAISS index using the FAISS.from_texts method, passing in the chunked_docs and embeddings.
# The FAISS.from_texts method creates a FAISS index that allows for efficient similarity search.
# After building the vector index, you can use it to retrieve relevant chunks for a given query

Now that you've built a FAISS index, the next step is to use it for retrieval and potentially integrate it with a language model for generation. Here are a few options:
# Similarity Search: You can use the FAISS index to perform similarity search and retrieve relevant chunks for a given query.
# Retrieval-Augmented Generation (RAG): You can integrate the FAISS index with a language model to generate responses based on the retrieved chunks.

# Next Steps
With the FAISS index built, you can proceed with further processing or analysis, such as:

### Similarity search: Using the FAISS index to find similar documents or chunks.
### Querying the index: Using a query string to search for relevant documents or chunks in the index.

You can now use the vector_store object to perform similarity search and retrieval. For example, you can use the similarity_search method to find similar documents or chunks.
"""

# ==============================================================================
# 7. Perform Similarity Search
# ==============================================================================

query = "What are the key elements of a contract?"
docs = vector_store.similarity_search(query, k=5)

print("Retrieved documents:")
for doc in docs:
    print(doc.page_content)

"""In this code:
We define a query and use the similarity_search method to retrieve the top 5 most similar chunks.
We print the retrieved chunks.
If you want to implement RAG, you'll need to integrate the FAISS index with a language model.

The similarity_search method is the core of the Retrieval component of your RAG system. It proves that your embeddings and FAISS index are working correctly by retrieving the most semantically relevant documents to your query.

You've successfully built the first part of your RAG pipeline. The final steps are to integrate the LLM for the "Generation" part and then combine everything into a single, cohesive chain.

Next Steps:

With the similarity search results, you can proceed with further processing or analysis, such as:

Summarizing the results: Summarizing the retrieved documents to provide a concise answer to the query.
Further filtering: Further filtering the retrieved documents based on additional criteria, such as relevance or accuracy.

we can also experiment with different queries and see how the similarity search results change.

The result shows that your Retrieval component is working perfectly. It has successfully taken your query, converted it into an embedding, and found the most semantically relevant legal document chunks from your FAISS index.

Now, we move on to the final and most exciting part: Generation. This is where we will use a Large Language Model (LLM) to read the retrieved documents and generate a human-like, concise answer based on that information.

This completes the full RAG pipeline.

Before that going for summarisation

This code snippet uses the Hugging Face Transformers library to summarize the first retrieved document.
"""

# Summarize the retrieved documents
from transformers import pipeline

summarizer = pipeline("summarization")

summary = summarizer(docs[0].page_content, max_length=100, min_length=5, do_sample=False)
print(summary)

"""# Using API token from hugging face and LLM Initialisation:

The below code block is a complete, step-by-step implementation of a Retrieval-Augmented Generation (RAG) system.

It's essentially showing an alternative way to build the final RAG chain that we were working on previously. Instead of using a high-level LangChain RetrievalQA chain, this code builds the pipeline manually, giving you more granular control.

# Indirect Method:

# **Creating API Token from hugging face**

---
"""

# Create API Token
#===============================================================================
import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your token" #token name from hugging face

"""# **LLM Initialization**

---


"""

llm = HuggingFaceHub(repo_id="google/flan-t5-base", huggingfacehub_api_token="your token")

!pip install -U langchain_huggingface

from langchain_huggingface import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    repo_id="google/flan-t5-base",
    max_new_tokens=512,
    temperature=0.1,
    huggingfacehub_api_token="your token"
)

"""Purpose: This section is all about setting up the Large Language Model (LLM) you will use for the "Generation" part of RAG.
================================================================================
What it does: It shows two different ways to use a Hugging Face model:

Environment Variable (os.environ): Sets your Hugging Face API token so other libraries can access it.

LangChain API Access (HuggingFaceHub & HuggingFaceEndpoint): These classes from LangChain allow you to access a model on the Hugging Face Hub (like flan-t5-base) via its API. HuggingFaceEndpoint is a more recent and configurable way to do this.

# 2. Manual RAG Implementation (Local Model Approach)

The rest of the code is a detailed, manual recreation of a RAG pipeline. This is a great way to understand exactly what the high-level RetrievalQA chain was doing behind the scenes.

We're importing the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library. These classes are used to load pre-trained models and tokenizers.

Step 1: Load the Model and Tokenizer

Step 2: Retrieve Relevant Documents

Step 3: Create a Context String

Step 4: Generate the Response:

Purpose: This is the Generation step, where the LLM produces the final answer.

What it does:

Builds the Final Prompt: It creates a well-structured prompt string that includes both the context (the retrieved documents) and the original query.

Tokenizes the Prompt: It uses the tokenizer to convert this combined prompt string into numerical tokens that the flan-t5 model can understand.

Generates an Answer: It feeds these tokens into the model.generate() function to produce a response.

Decodes the Output: It then uses the tokenizer to convert the model's numerical output back into a human-readable text string (response).

In summary, this code block demonstrates a working RAG pipeline built from its fundamental components. It's a great way to understand the flow from retrieval to generation without relying on a single, high-level abstraction like the RetrievalQA chain.
"""

# ==============================================================================
# 8. Generate Response
# ==============================================================================
# Importing the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library.
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Step 1: Load the Model and Tokenizer
# We're loading a pre-trained model called flan-t5-base and its corresponding tokenizer.
#===============================================================================
# Load the model and tokenizer
#===============================================================================
model_name = "google/flan-t5-base"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Purpose: Instead of using an API, this loads the actual flan-t5-base model and its tokenizer onto your local machine (or Colab GPU).
# This gives you more control but requires more memory.

## Step 2: Retrieve Relevant Documents
#*******************************************************************************
# We're defining a query about the key elements of a contract and using a vector_store to search for relevant documents.
#===============================================================================
# Retrieve relevant documents
#===============================================================================
query = "What are the key elements of a contract?"
docs = vector_store.similarity_search(query, k=5)
# Purpose: This is the Retrieval step.

# What it does:
# It takes a query, converts it into a vector, and then uses your vector_store (the FAISS index you built) to find the 5 most similar document chunks (k=5).

#Step 3: Create a Context String
#*******************************************************************************
# We're creating a context string by concatenating the contents of the relevant documents.
#===============================================================================
# Create a context string
#===============================================================================
context = ""
for doc in docs:
    context += doc.page_content + "\n\n"
    if len(context) > 2000:  # truncate context to prevent excessive length
        break
# Purpose: To prepare the retrieved documents for the LLM.

# What it does:
# It loops through the retrieved documents (docs) and concatenates their text content (doc.page_content) into a single string called context. This is the information the LLM will use to answer the question. It also includes a safety check to prevent the context from getting too long.

#Step 4: Generate the Response
#*******************************************************************************
# Generate response
prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
output = model.generate(inputs["input_ids"], max_new_tokens=512)

response = tokenizer.decode(output[0], skip_special_tokens=True)

print("Generated response:")
print(response)

"""Purpose: This is the Generation step, where the LLM produces the final answer.

What it does:

Builds the Final Prompt: It creates a well-structured prompt string that includes both the context (the retrieved documents) and the original query.

Tokenizes the Prompt: It uses the tokenizer to convert this combined prompt string into numerical tokens that the flan-t5 model can understand.

Generates an Answer: It feeds these tokens into the model.generate() function to produce a response.

Decodes the Output: It then uses the tokenizer to convert the model's numerical output back into a human-readable text string (response).

In summary, this code block demonstrates a working RAG pipeline built from its fundamental components. It's a great way to understand the flow from retrieval to generation without relying on a single, high-level abstraction like the RetrievalQA chain.








"""

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the model and tokenizer
model_name = "google/flan-t5-base"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Retrieve relevant documents
query = "What are the key elements of a contract?"
docs = vector_store.similarity_search(query, k=5)

# Create a context string
context = ""
for doc in docs:
    context += doc.page_content + "\n\n"
    if len(context) > 2000:  # truncate context to prevent excessive length
        break

# Generate response
prompt = f"Based on general knowledge of contracts, answer the following question: {query}. Provide a detailed response."
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
output = model.generate(inputs["input_ids"], max_new_tokens=1024, num_beams=4)

response = tokenizer.decode(output[0], skip_special_tokens=True)

print("Generated response:")
print(response)

"""This not a good response .so trying further."""

from transformers import pipeline

# Load the model and tokenizer
generator = pipeline('text-generation', model='gpt2-medium')


# We're defining a query about the key elements of a contract and using a vector_store to search for relevant documents.
#===============================================================================
# Retrieve relevant documents
#===============================================================================
query = "What are the key elements of a contract?"
docs = vector_store.similarity_search(query, k=5)


#  We're creating a context string by concatenating the contents of the relevant documents.
#===============================================================================
# Create a context string
#===============================================================================
context = ""
for doc in docs:
    context += doc.page_content + "\n\n"
    if len(context) > 2000:  # truncate context to prevent excessive length
        break

# We're defining a prompt and using the tokenizer to convert it into input IDs. We're then generating output using the model.
#================================================================================
# Generate response
#===============================================================================
prompt = f"What are the key elements of a contract? Consider offer, acceptance, consideration, and other essential components. Provide a detailed response."
response = generator(prompt, max_length=1024, num_return_sequences=1)

print("Generated response:")
print(response[0]['generated_text'])

"""The generated response provides some useful information about contracts, but it seems to be a bit disjointed and doesn't directly address the key elements of a contract in a clear and concise manner.
Let's try to refine the prompt and see if we can get a more focused response:
"""

from transformers import pipeline

# Load the model and tokenizer
generator = pipeline('text-generation', model='gpt2-medium')

# Generate response
prompt = "The key elements of a contract include offer, acceptance, and consideration. Other essential components are"
response = generator(prompt, max_length=256, num_return_sequences=1)

print("Generated response:")
print(response[0]['generated_text'])

"""The generated response provides some useful information about contracts, but it seems to be repetitive and doesn't provide a clear and concise overview of the key elements of a contract.
Let's try to refine the prompt and model parameters to get a better response:
"""

from transformers import pipeline

# Load the model and tokenizer
generator = pipeline('text-generation', model='gpt2-medium', max_length=512)

# Generate response
prompt = "The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail."
response = generator(prompt, num_return_sequences=1, truncation=True)

print("Generated response:")
print(response[0]['generated_text'])

"""The generated response seems to be cut off and doesn't provide a detailed explanation of the key elements of a contract.
Let's try to adjust the model parameters to generate a longer and more comprehensive response:

# In this code:
We're setting device=-1 to load the model on the CPU instead of the CUDA device.
# This should prevent the CUDA error and allow the model to generate a response.

The below code performs direct text generation using a pre-trained language model from the Hugging Face transformers library.

Unlike the RAG pipeline we were building, this code does not retrieve any information from an external document base. Instead, it relies solely on the internal knowledge of the model itself to generate a response to the prompt.

# 1. Model and Pipeline Loading-

from transformers import pipeline: This imports the high-level pipeline function, which is a convenient way to use pre-trained models for specific tasks without needing to manually handle tokenization, model inference, and output processing.

generator = pipeline(...): This line initializes a text generation pipeline.

'text-generation' specifies the task you want to perform.

model='gpt2-medium' tells the pipeline to use the gpt2-medium model from the Hugging Face Hub. It will automatically download the model weights and its associated tokenizer.

device=-1 specifies that the model should be loaded and run on the CPU. If you had a GPU available, you would use device=0 (or device=1, etc.) to use it for faster performance.
# 2. Defining the Prompt and Generating Response

prompt = "...": This defines the initial text that the model will use as a starting point. The model will try to continue this text in a coherent and relevant way.

response = generator(...): This is the core command that generates the text.

The first argument, prompt, is the input text.

max_new_tokens=512 sets the maximum number of new tokens (words or subwords) that the model should generate.

num_return_sequences=1 specifies that the model should generate only one output text sequence. You could increase this to, for example, 3 to get three different generated responses for the same prompt.

# 3. Printing the Response

print(response[0]['generated_text']): The generator() function returns a list of dictionaries. Each dictionary contains the generated text under the key 'generated_text'. This line accesses the first (and in this case, only) dictionary in the list and prints the generated text.
"""

from transformers import pipeline

# 1. Model and Pipeline Loading
# We're loading a pre-trained model called GPT-2 MEDIUM and its corresponding tokenizer.
#===============================================================================
# Load the model and tokenizer
#===============================================================================
generator = pipeline('text-generation', model='gpt2-medium', device=-1)
# This line loads a pre-trained GPT-2 model with the text-generation pipeline. The device=-1 parameter specifies that the model should be loaded on the CPU (instead of a GPU).

# 2. Defining the Prompt and Generating Response
#This line defines the prompt that will be used to generate text.
#===============================================================================
# Defining the Prompt and Generating Response:
#===============================================================================
prompt = "The key elements of a contract include offer, acceptance, and consideration. Explain these components and other essential parts of a contract in detail."
response = generator(prompt, max_new_tokens=512, num_return_sequences=1) # This line generates text based on the prompt. The max_new_tokens=512 parameter specifies the maximum number of new tokens to generate, and num_return_sequences=1 specifies that only one sequence of text should be generated.

# 3. Printing the Response
# Printing the response:
#================================================================================
print("Generated response:")
print(response[0]['generated_text'])
# This line prints the generated text.

"""The generated response provides a detailed and comprehensive overview of contract essentials, including payment terms, provisions for withdrawal, and settlement negotiations.
It seems like the model was able to generate a lengthy and coherent response that covers various aspects of contracts.

Model Details
==============
Model Name: GPT-2 Medium
Model Type: Transformer-based language model
Use Case: Text generation

Parameters
====================
max_new_tokens: The maximum number of new tokens to generate. In this case, it's set to 512.
num_return_sequences: The number of sequences to generate. In this case, it's set to 1.

Output
====================
The code will output the generated text based on the prompt. The text will explain the key elements of a contract, including offer, acceptance, and consideration, as well as other essential parts of a contract.

==============================================================================
# LLM Setup (for the Generation step of RAG)
# ==============================================================================

For this step, we will use a small, open-source LLM that can run on Colab.
A great choice is a quantized version of a model from the Hugging Face Hub
to save GPU memory. Here, we'll use a version of 'mistralai/Mistral-7B-Instruct-v0.2'.
Make sure you have a Hugging Face Hub token set up in Colab for access.
If you don't have a token, you can also use `LlamaCpp` or a simple QA model,
but a chat model gives better conversational results.

# To set up your token: Go to Hugging Face, create an account, get an access token,
# and add it in Colab Secrets or a prompt.
# from google.colab import userdata
# HF_TOKEN = userdata.get('HF_TOKEN')
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

# **Direct method:**Using RetrievalQA

---
"""

# ==============================================================================
# 8.LLM Setup (for the Generation step of RAG)
# ==============================================================================
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

# We are using an openly accessible model.
llm_model_name = "google/flan-t5-base"
print(f"\nLoading the LLM: {llm_model_name}")

# Create a transformers pipeline for text-generation
# We pass in the model and tokenizer to the pipeline
pipe = pipeline(
    "text2text-generation",
    model=AutoModelForSeq2SeqLM.from_pretrained(llm_model_name),
    tokenizer=AutoTokenizer.from_pretrained(llm_model_name),
    max_length=512,
    device_map="auto"
)

# Now, wrap the transformers pipeline in the HuggingFacePipeline class.
# This makes it compatible with LangChain's RetrievalQA.
llm = HuggingFacePipeline(pipeline=pipe)
print("LLM loaded and ready.")


# ==============================================================================
# 9. Building the RAG Chain and Querying
# ==============================================================================

# A custom prompt helps guide the LLM to provide a relevant, concise answer.
prompt_template = """Use the following pieces of context to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
{context}
----------------
Question: {question}
Answer:"""

# Create a prompt instance with the template.
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# Initialize the RAG chain. This chain orchestrates the entire RAG process.
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(),
    chain_type_kwargs={"prompt": PROMPT}
)

# ==============================================================================
# 10. Example Queries
# ==============================================================================

print("\n--- RAG System Ready ---")
print("Enter your questions about the legal documents. Type 'exit' to quit.")

while True:
    user_query = input("\nYour question: ")
    if user_query.lower() == 'exit':
        print("Exiting. Goodbye!")
        break

    print("Searching for answer...")

    response = rag_chain.invoke(user_query)

    print(f"\nAI Response: {response['result']}")

# ==============================================================================
# 5. Building the RAG Chain and Querying
# ==============================================================================

# A custom prompt helps guide the LLM to provide a relevant, concise answer.
prompt_template = """Use the following pieces of context to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
{context}
----------------
Question: {question}
Answer:"""

# Create a prompt instance with the template.
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# Initialize the RAG chain. This chain orchestrates the entire RAG process.
# We will now pass search_kwargs to the retriever to retrieve fewer documents.
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    # Pass search_kwargs to the retriever to only get the top 1 most relevant document.
    # This helps avoid retrieving irrelevant information.
    retriever=vector_store.as_retriever(search_kwargs={"k": 1}),
    chain_type_kwargs={"prompt": PROMPT}
)

# ==============================================================================
# 6. Example Queries
# ==============================================================================

print("\n--- RAG System Ready ---")
print("Enter your questions about the legal documents. Type 'exit' to quit.")

while True:
    user_query = input("\nYour question: ")
    if user_query.lower() == 'exit':
        print("Exiting. Goodbye!")
        break

    print("Searching for answer...")

    response = rag_chain.invoke(user_query)

    print(f"\nAI Response: {response['result']}")

"""The new output is much better! It shows that the fix of setting k=1 worked perfectly. The system is no longer combining two different document chunks. The model has correctly retrieved a single, relevant piece of information.

What is Happening Now
The new problem is with the Generation step. The flan-t5-base model is designed for a variety of tasks, but it is not the best at instruction-following or complex summarization. It is essentially taking the document snippet and re-writing it, but it isn't truly "explaining" the concept. The model is too "passive."

How to Fix It
This is a classic case of needing better prompt engineering or a more capable LLM. We'll try to solve this with a simple prompt change first.

The current prompt asks the model to "answer the user's question," which can be interpreted by a model like Flan-T5 as simply reproducing the context.
"""